{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next word prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/san5627/Next-Word-Prediction-using-NLP/blob/main/Next_word_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iP7akNWjC7m"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ_pNi7TjEsl"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlZja-vGjEga",
        "outputId": "afbfbbd0-29e9-4f73-a1de-57d919e5ef95"
      },
      "source": [
        "# load text\n",
        "raw_text = load_doc('/content/rhyme.txt')\n",
        "print(raw_text)\n",
        "\n",
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence,\n",
            "A pocket full of rye.\n",
            "Four and twenty blackbirds,\n",
            "Baked in a pie.\n",
            "\n",
            "When the pie was opened\n",
            "The birds began to sing;\n",
            "Wasn't that a dainty dish,\n",
            "To set before the king.\n",
            "\n",
            "The king was in his counting house,\n",
            "Counting out his money;\n",
            "The queen was in the parlour,\n",
            "Eating bread and honey.\n",
            "\n",
            "The maid was in the garden,\n",
            "Hanging out the clothes,\n",
            "When down came a blackbird\n",
            "And pecked off her nose.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnAsq0hKjES0",
        "outputId": "0027e5f1-d874-4130-d1c5-abcef899fad1"
      },
      "source": [
        "# organize into sequences of characters\n",
        "length = 10\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = raw_text[i-length:i+1]\n",
        "\t# store\n",
        "\tsequences.append(seq)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTXfiwrJjfsI",
        "outputId": "71d01895-8085-448e-e34b-c85126d4a072"
      },
      "source": [
        "sequences"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sing a song',\n",
              " 'ing a song ',\n",
              " 'ng a song o',\n",
              " 'g a song of',\n",
              " ' a song of ',\n",
              " 'a song of s',\n",
              " ' song of si',\n",
              " 'song of six',\n",
              " 'ong of sixp',\n",
              " 'ng of sixpe',\n",
              " 'g of sixpen',\n",
              " ' of sixpenc',\n",
              " 'of sixpence',\n",
              " 'f sixpence,',\n",
              " ' sixpence, ',\n",
              " 'sixpence, A',\n",
              " 'ixpence, A ',\n",
              " 'xpence, A p',\n",
              " 'pence, A po',\n",
              " 'ence, A poc',\n",
              " 'nce, A pock',\n",
              " 'ce, A pocke',\n",
              " 'e, A pocket',\n",
              " ', A pocket ',\n",
              " ' A pocket f',\n",
              " 'A pocket fu',\n",
              " ' pocket ful',\n",
              " 'pocket full',\n",
              " 'ocket full ',\n",
              " 'cket full o',\n",
              " 'ket full of',\n",
              " 'et full of ',\n",
              " 't full of r',\n",
              " ' full of ry',\n",
              " 'full of rye',\n",
              " 'ull of rye.',\n",
              " 'll of rye. ',\n",
              " 'l of rye. F',\n",
              " ' of rye. Fo',\n",
              " 'of rye. Fou',\n",
              " 'f rye. Four',\n",
              " ' rye. Four ',\n",
              " 'rye. Four a',\n",
              " 'ye. Four an',\n",
              " 'e. Four and',\n",
              " '. Four and ',\n",
              " ' Four and t',\n",
              " 'Four and tw',\n",
              " 'our and twe',\n",
              " 'ur and twen',\n",
              " 'r and twent',\n",
              " ' and twenty',\n",
              " 'and twenty ',\n",
              " 'nd twenty b',\n",
              " 'd twenty bl',\n",
              " ' twenty bla',\n",
              " 'twenty blac',\n",
              " 'wenty black',\n",
              " 'enty blackb',\n",
              " 'nty blackbi',\n",
              " 'ty blackbir',\n",
              " 'y blackbird',\n",
              " ' blackbirds',\n",
              " 'blackbirds,',\n",
              " 'lackbirds, ',\n",
              " 'ackbirds, B',\n",
              " 'ckbirds, Ba',\n",
              " 'kbirds, Bak',\n",
              " 'birds, Bake',\n",
              " 'irds, Baked',\n",
              " 'rds, Baked ',\n",
              " 'ds, Baked i',\n",
              " 's, Baked in',\n",
              " ', Baked in ',\n",
              " ' Baked in a',\n",
              " 'Baked in a ',\n",
              " 'aked in a p',\n",
              " 'ked in a pi',\n",
              " 'ed in a pie',\n",
              " 'd in a pie.',\n",
              " ' in a pie. ',\n",
              " 'in a pie. W',\n",
              " 'n a pie. Wh',\n",
              " ' a pie. Whe',\n",
              " 'a pie. When',\n",
              " ' pie. When ',\n",
              " 'pie. When t',\n",
              " 'ie. When th',\n",
              " 'e. When the',\n",
              " '. When the ',\n",
              " ' When the p',\n",
              " 'When the pi',\n",
              " 'hen the pie',\n",
              " 'en the pie ',\n",
              " 'n the pie w',\n",
              " ' the pie wa',\n",
              " 'the pie was',\n",
              " 'he pie was ',\n",
              " 'e pie was o',\n",
              " ' pie was op',\n",
              " 'pie was ope',\n",
              " 'ie was open',\n",
              " 'e was opene',\n",
              " ' was opened',\n",
              " 'was opened ',\n",
              " 'as opened T',\n",
              " 's opened Th',\n",
              " ' opened The',\n",
              " 'opened The ',\n",
              " 'pened The b',\n",
              " 'ened The bi',\n",
              " 'ned The bir',\n",
              " 'ed The bird',\n",
              " 'd The birds',\n",
              " ' The birds ',\n",
              " 'The birds b',\n",
              " 'he birds be',\n",
              " 'e birds beg',\n",
              " ' birds bega',\n",
              " 'birds began',\n",
              " 'irds began ',\n",
              " 'rds began t',\n",
              " 'ds began to',\n",
              " 's began to ',\n",
              " ' began to s',\n",
              " 'began to si',\n",
              " 'egan to sin',\n",
              " 'gan to sing',\n",
              " 'an to sing;',\n",
              " 'n to sing; ',\n",
              " ' to sing; W',\n",
              " 'to sing; Wa',\n",
              " 'o sing; Was',\n",
              " ' sing; Wasn',\n",
              " \"sing; Wasn'\",\n",
              " \"ing; Wasn't\",\n",
              " \"ng; Wasn't \",\n",
              " \"g; Wasn't t\",\n",
              " \"; Wasn't th\",\n",
              " \" Wasn't tha\",\n",
              " \"Wasn't that\",\n",
              " \"asn't that \",\n",
              " \"sn't that a\",\n",
              " \"n't that a \",\n",
              " \"'t that a d\",\n",
              " 't that a da',\n",
              " ' that a dai',\n",
              " 'that a dain',\n",
              " 'hat a daint',\n",
              " 'at a dainty',\n",
              " 't a dainty ',\n",
              " ' a dainty d',\n",
              " 'a dainty di',\n",
              " ' dainty dis',\n",
              " 'dainty dish',\n",
              " 'ainty dish,',\n",
              " 'inty dish, ',\n",
              " 'nty dish, T',\n",
              " 'ty dish, To',\n",
              " 'y dish, To ',\n",
              " ' dish, To s',\n",
              " 'dish, To se',\n",
              " 'ish, To set',\n",
              " 'sh, To set ',\n",
              " 'h, To set b',\n",
              " ', To set be',\n",
              " ' To set bef',\n",
              " 'To set befo',\n",
              " 'o set befor',\n",
              " ' set before',\n",
              " 'set before ',\n",
              " 'et before t',\n",
              " 't before th',\n",
              " ' before the',\n",
              " 'before the ',\n",
              " 'efore the k',\n",
              " 'fore the ki',\n",
              " 'ore the kin',\n",
              " 're the king',\n",
              " 'e the king.',\n",
              " ' the king. ',\n",
              " 'the king. T',\n",
              " 'he king. Th',\n",
              " 'e king. The',\n",
              " ' king. The ',\n",
              " 'king. The k',\n",
              " 'ing. The ki',\n",
              " 'ng. The kin',\n",
              " 'g. The king',\n",
              " '. The king ',\n",
              " ' The king w',\n",
              " 'The king wa',\n",
              " 'he king was',\n",
              " 'e king was ',\n",
              " ' king was i',\n",
              " 'king was in',\n",
              " 'ing was in ',\n",
              " 'ng was in h',\n",
              " 'g was in hi',\n",
              " ' was in his',\n",
              " 'was in his ',\n",
              " 'as in his c',\n",
              " 's in his co',\n",
              " ' in his cou',\n",
              " 'in his coun',\n",
              " 'n his count',\n",
              " ' his counti',\n",
              " 'his countin',\n",
              " 'is counting',\n",
              " 's counting ',\n",
              " ' counting h',\n",
              " 'counting ho',\n",
              " 'ounting hou',\n",
              " 'unting hous',\n",
              " 'nting house',\n",
              " 'ting house,',\n",
              " 'ing house, ',\n",
              " 'ng house, C',\n",
              " 'g house, Co',\n",
              " ' house, Cou',\n",
              " 'house, Coun',\n",
              " 'ouse, Count',\n",
              " 'use, Counti',\n",
              " 'se, Countin',\n",
              " 'e, Counting',\n",
              " ', Counting ',\n",
              " ' Counting o',\n",
              " 'Counting ou',\n",
              " 'ounting out',\n",
              " 'unting out ',\n",
              " 'nting out h',\n",
              " 'ting out hi',\n",
              " 'ing out his',\n",
              " 'ng out his ',\n",
              " 'g out his m',\n",
              " ' out his mo',\n",
              " 'out his mon',\n",
              " 'ut his mone',\n",
              " 't his money',\n",
              " ' his money;',\n",
              " 'his money; ',\n",
              " 'is money; T',\n",
              " 's money; Th',\n",
              " ' money; The',\n",
              " 'money; The ',\n",
              " 'oney; The q',\n",
              " 'ney; The qu',\n",
              " 'ey; The que',\n",
              " 'y; The quee',\n",
              " '; The queen',\n",
              " ' The queen ',\n",
              " 'The queen w',\n",
              " 'he queen wa',\n",
              " 'e queen was',\n",
              " ' queen was ',\n",
              " 'queen was i',\n",
              " 'ueen was in',\n",
              " 'een was in ',\n",
              " 'en was in t',\n",
              " 'n was in th',\n",
              " ' was in the',\n",
              " 'was in the ',\n",
              " 'as in the p',\n",
              " 's in the pa',\n",
              " ' in the par',\n",
              " 'in the parl',\n",
              " 'n the parlo',\n",
              " ' the parlou',\n",
              " 'the parlour',\n",
              " 'he parlour,',\n",
              " 'e parlour, ',\n",
              " ' parlour, E',\n",
              " 'parlour, Ea',\n",
              " 'arlour, Eat',\n",
              " 'rlour, Eati',\n",
              " 'lour, Eatin',\n",
              " 'our, Eating',\n",
              " 'ur, Eating ',\n",
              " 'r, Eating b',\n",
              " ', Eating br',\n",
              " ' Eating bre',\n",
              " 'Eating brea',\n",
              " 'ating bread',\n",
              " 'ting bread ',\n",
              " 'ing bread a',\n",
              " 'ng bread an',\n",
              " 'g bread and',\n",
              " ' bread and ',\n",
              " 'bread and h',\n",
              " 'read and ho',\n",
              " 'ead and hon',\n",
              " 'ad and hone',\n",
              " 'd and honey',\n",
              " ' and honey.',\n",
              " 'and honey. ',\n",
              " 'nd honey. T',\n",
              " 'd honey. Th',\n",
              " ' honey. The',\n",
              " 'honey. The ',\n",
              " 'oney. The m',\n",
              " 'ney. The ma',\n",
              " 'ey. The mai',\n",
              " 'y. The maid',\n",
              " '. The maid ',\n",
              " ' The maid w',\n",
              " 'The maid wa',\n",
              " 'he maid was',\n",
              " 'e maid was ',\n",
              " ' maid was i',\n",
              " 'maid was in',\n",
              " 'aid was in ',\n",
              " 'id was in t',\n",
              " 'd was in th',\n",
              " ' was in the',\n",
              " 'was in the ',\n",
              " 'as in the g',\n",
              " 's in the ga',\n",
              " ' in the gar',\n",
              " 'in the gard',\n",
              " 'n the garde',\n",
              " ' the garden',\n",
              " 'the garden,',\n",
              " 'he garden, ',\n",
              " 'e garden, H',\n",
              " ' garden, Ha',\n",
              " 'garden, Han',\n",
              " 'arden, Hang',\n",
              " 'rden, Hangi',\n",
              " 'den, Hangin',\n",
              " 'en, Hanging',\n",
              " 'n, Hanging ',\n",
              " ', Hanging o',\n",
              " ' Hanging ou',\n",
              " 'Hanging out',\n",
              " 'anging out ',\n",
              " 'nging out t',\n",
              " 'ging out th',\n",
              " 'ing out the',\n",
              " 'ng out the ',\n",
              " 'g out the c',\n",
              " ' out the cl',\n",
              " 'out the clo',\n",
              " 'ut the clot',\n",
              " 't the cloth',\n",
              " ' the clothe',\n",
              " 'the clothes',\n",
              " 'he clothes,',\n",
              " 'e clothes, ',\n",
              " ' clothes, W',\n",
              " 'clothes, Wh',\n",
              " 'lothes, Whe',\n",
              " 'othes, When',\n",
              " 'thes, When ',\n",
              " 'hes, When d',\n",
              " 'es, When do',\n",
              " 's, When dow',\n",
              " ', When down',\n",
              " ' When down ',\n",
              " 'When down c',\n",
              " 'hen down ca',\n",
              " 'en down cam',\n",
              " 'n down came',\n",
              " ' down came ',\n",
              " 'down came a',\n",
              " 'own came a ',\n",
              " 'wn came a b',\n",
              " 'n came a bl',\n",
              " ' came a bla',\n",
              " 'came a blac',\n",
              " 'ame a black',\n",
              " 'me a blackb',\n",
              " 'e a blackbi',\n",
              " ' a blackbir',\n",
              " 'a blackbird',\n",
              " ' blackbird ',\n",
              " 'blackbird A',\n",
              " 'lackbird An',\n",
              " 'ackbird And',\n",
              " 'ckbird And ',\n",
              " 'kbird And p',\n",
              " 'bird And pe',\n",
              " 'ird And pec',\n",
              " 'rd And peck',\n",
              " 'd And pecke',\n",
              " ' And pecked',\n",
              " 'And pecked ',\n",
              " 'nd pecked o',\n",
              " 'd pecked of',\n",
              " ' pecked off',\n",
              " 'pecked off ',\n",
              " 'ecked off h',\n",
              " 'cked off he',\n",
              " 'ked off her',\n",
              " 'ed off her ',\n",
              " 'd off her n',\n",
              " ' off her no',\n",
              " 'off her nos',\n",
              " 'ff her nose',\n",
              " 'f her nose.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgTU3eJZGStB"
      },
      "source": [
        "# save sequences to file\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZqNWk3aFxCe"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xLHiwNYjrgN"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFrFutp2jrnS",
        "outputId": "92047610-bcf4-400e-f9be-36a1f4136c43"
      },
      "source": [
        "# integer encode sequences of characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "print(chars)\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "print(mapping)\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "\t# integer encode line\n",
        "\tencoded_seq = [mapping[char] for char in line]\n",
        "\t# store\n",
        "\tsequences.append(encoded_seq)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', \"'\", ',', '.', ';', 'A', 'B', 'C', 'E', 'F', 'H', 'S', 'T', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'w', 'x', 'y']\n",
            "{'\\n': 0, ' ': 1, \"'\": 2, ',': 3, '.': 4, ';': 5, 'A': 6, 'B': 7, 'C': 8, 'E': 9, 'F': 10, 'H': 11, 'S': 12, 'T': 13, 'W': 14, 'a': 15, 'b': 16, 'c': 17, 'd': 18, 'e': 19, 'f': 20, 'g': 21, 'h': 22, 'i': 23, 'k': 24, 'l': 25, 'm': 26, 'n': 27, 'o': 28, 'p': 29, 'q': 30, 'r': 31, 's': 32, 't': 33, 'u': 34, 'w': 35, 'x': 36, 'y': 37}\n",
            "Vocabulary Size: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzhpqh8xjrqo"
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aZc6s1j7kbL",
        "outputId": "ab6e1c37-0549-4882-df46-89004ede7bca"
      },
      "source": [
        "print(X)\n",
        "print(\"------------\")\n",
        "print(y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "------------\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSODRU8kj33g",
        "outputId": "5080f110-8f47-45c1-e610-c0b7441be3a4"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 75)                34200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 38)                2888      \n",
            "=================================================================\n",
            "Total params: 37,088\n",
            "Trainable params: 37,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "13/13 - 18s - loss: 3.6090 - accuracy: 0.1103\n",
            "Epoch 2/100\n",
            "13/13 - 0s - loss: 3.4864 - accuracy: 0.1930\n",
            "Epoch 3/100\n",
            "13/13 - 0s - loss: 3.1965 - accuracy: 0.1905\n",
            "Epoch 4/100\n",
            "13/13 - 0s - loss: 3.0368 - accuracy: 0.1905\n",
            "Epoch 5/100\n",
            "13/13 - 0s - loss: 3.0116 - accuracy: 0.1905\n",
            "Epoch 6/100\n",
            "13/13 - 0s - loss: 2.9877 - accuracy: 0.1905\n",
            "Epoch 7/100\n",
            "13/13 - 0s - loss: 2.9679 - accuracy: 0.1905\n",
            "Epoch 8/100\n",
            "13/13 - 0s - loss: 2.9566 - accuracy: 0.1905\n",
            "Epoch 9/100\n",
            "13/13 - 0s - loss: 2.9426 - accuracy: 0.1905\n",
            "Epoch 10/100\n",
            "13/13 - 0s - loss: 2.9235 - accuracy: 0.1905\n",
            "Epoch 11/100\n",
            "13/13 - 0s - loss: 2.9065 - accuracy: 0.1905\n",
            "Epoch 12/100\n",
            "13/13 - 0s - loss: 2.8881 - accuracy: 0.1905\n",
            "Epoch 13/100\n",
            "13/13 - 0s - loss: 2.8627 - accuracy: 0.2105\n",
            "Epoch 14/100\n",
            "13/13 - 0s - loss: 2.8332 - accuracy: 0.1905\n",
            "Epoch 15/100\n",
            "13/13 - 0s - loss: 2.8048 - accuracy: 0.2306\n",
            "Epoch 16/100\n",
            "13/13 - 0s - loss: 2.7678 - accuracy: 0.2155\n",
            "Epoch 17/100\n",
            "13/13 - 0s - loss: 2.7335 - accuracy: 0.2256\n",
            "Epoch 18/100\n",
            "13/13 - 0s - loss: 2.6903 - accuracy: 0.2657\n",
            "Epoch 19/100\n",
            "13/13 - 0s - loss: 2.6555 - accuracy: 0.2807\n",
            "Epoch 20/100\n",
            "13/13 - 0s - loss: 2.6113 - accuracy: 0.2682\n",
            "Epoch 21/100\n",
            "13/13 - 0s - loss: 2.5716 - accuracy: 0.2907\n",
            "Epoch 22/100\n",
            "13/13 - 0s - loss: 2.5441 - accuracy: 0.2707\n",
            "Epoch 23/100\n",
            "13/13 - 0s - loss: 2.4917 - accuracy: 0.3008\n",
            "Epoch 24/100\n",
            "13/13 - 0s - loss: 2.4481 - accuracy: 0.2982\n",
            "Epoch 25/100\n",
            "13/13 - 0s - loss: 2.3956 - accuracy: 0.3208\n",
            "Epoch 26/100\n",
            "13/13 - 0s - loss: 2.3594 - accuracy: 0.3308\n",
            "Epoch 27/100\n",
            "13/13 - 0s - loss: 2.3244 - accuracy: 0.3208\n",
            "Epoch 28/100\n",
            "13/13 - 0s - loss: 2.2821 - accuracy: 0.3133\n",
            "Epoch 29/100\n",
            "13/13 - 0s - loss: 2.2285 - accuracy: 0.3559\n",
            "Epoch 30/100\n",
            "13/13 - 0s - loss: 2.1785 - accuracy: 0.3835\n",
            "Epoch 31/100\n",
            "13/13 - 0s - loss: 2.1329 - accuracy: 0.4010\n",
            "Epoch 32/100\n",
            "13/13 - 0s - loss: 2.1010 - accuracy: 0.3709\n",
            "Epoch 33/100\n",
            "13/13 - 0s - loss: 2.0479 - accuracy: 0.4261\n",
            "Epoch 34/100\n",
            "13/13 - 0s - loss: 2.0403 - accuracy: 0.4185\n",
            "Epoch 35/100\n",
            "13/13 - 0s - loss: 1.9669 - accuracy: 0.4637\n",
            "Epoch 36/100\n",
            "13/13 - 0s - loss: 1.9713 - accuracy: 0.4336\n",
            "Epoch 37/100\n",
            "13/13 - 0s - loss: 1.8939 - accuracy: 0.4762\n",
            "Epoch 38/100\n",
            "13/13 - 0s - loss: 1.8655 - accuracy: 0.4637\n",
            "Epoch 39/100\n",
            "13/13 - 0s - loss: 1.8355 - accuracy: 0.4887\n",
            "Epoch 40/100\n",
            "13/13 - 0s - loss: 1.7937 - accuracy: 0.5263\n",
            "Epoch 41/100\n",
            "13/13 - 0s - loss: 1.7546 - accuracy: 0.5113\n",
            "Epoch 42/100\n",
            "13/13 - 0s - loss: 1.7077 - accuracy: 0.5213\n",
            "Epoch 43/100\n",
            "13/13 - 0s - loss: 1.6494 - accuracy: 0.5564\n",
            "Epoch 44/100\n",
            "13/13 - 0s - loss: 1.6305 - accuracy: 0.5514\n",
            "Epoch 45/100\n",
            "13/13 - 0s - loss: 1.6060 - accuracy: 0.5514\n",
            "Epoch 46/100\n",
            "13/13 - 0s - loss: 1.5665 - accuracy: 0.5489\n",
            "Epoch 47/100\n",
            "13/13 - 0s - loss: 1.5053 - accuracy: 0.5915\n",
            "Epoch 48/100\n",
            "13/13 - 0s - loss: 1.4753 - accuracy: 0.5990\n",
            "Epoch 49/100\n",
            "13/13 - 0s - loss: 1.4368 - accuracy: 0.6140\n",
            "Epoch 50/100\n",
            "13/13 - 0s - loss: 1.4188 - accuracy: 0.6190\n",
            "Epoch 51/100\n",
            "13/13 - 0s - loss: 1.3979 - accuracy: 0.6216\n",
            "Epoch 52/100\n",
            "13/13 - 0s - loss: 1.3690 - accuracy: 0.6491\n",
            "Epoch 53/100\n",
            "13/13 - 0s - loss: 1.3471 - accuracy: 0.6466\n",
            "Epoch 54/100\n",
            "13/13 - 0s - loss: 1.2896 - accuracy: 0.6867\n",
            "Epoch 55/100\n",
            "13/13 - 0s - loss: 1.2418 - accuracy: 0.6842\n",
            "Epoch 56/100\n",
            "13/13 - 0s - loss: 1.1972 - accuracy: 0.7043\n",
            "Epoch 57/100\n",
            "13/13 - 0s - loss: 1.1953 - accuracy: 0.7018\n",
            "Epoch 58/100\n",
            "13/13 - 0s - loss: 1.1788 - accuracy: 0.7093\n",
            "Epoch 59/100\n",
            "13/13 - 0s - loss: 1.0927 - accuracy: 0.7469\n",
            "Epoch 60/100\n",
            "13/13 - 0s - loss: 1.0526 - accuracy: 0.7619\n",
            "Epoch 61/100\n",
            "13/13 - 0s - loss: 1.0330 - accuracy: 0.7619\n",
            "Epoch 62/100\n",
            "13/13 - 0s - loss: 1.0012 - accuracy: 0.7769\n",
            "Epoch 63/100\n",
            "13/13 - 0s - loss: 0.9828 - accuracy: 0.7769\n",
            "Epoch 64/100\n",
            "13/13 - 0s - loss: 0.9623 - accuracy: 0.7895\n",
            "Epoch 65/100\n",
            "13/13 - 0s - loss: 0.9136 - accuracy: 0.8045\n",
            "Epoch 66/100\n",
            "13/13 - 0s - loss: 0.8889 - accuracy: 0.8070\n",
            "Epoch 67/100\n",
            "13/13 - 0s - loss: 0.8498 - accuracy: 0.8321\n",
            "Epoch 68/100\n",
            "13/13 - 0s - loss: 0.8385 - accuracy: 0.8221\n",
            "Epoch 69/100\n",
            "13/13 - 0s - loss: 0.8121 - accuracy: 0.8421\n",
            "Epoch 70/100\n",
            "13/13 - 0s - loss: 0.7922 - accuracy: 0.8371\n",
            "Epoch 71/100\n",
            "13/13 - 0s - loss: 0.7550 - accuracy: 0.8596\n",
            "Epoch 72/100\n",
            "13/13 - 0s - loss: 0.7456 - accuracy: 0.8471\n",
            "Epoch 73/100\n",
            "13/13 - 0s - loss: 0.7131 - accuracy: 0.8747\n",
            "Epoch 74/100\n",
            "13/13 - 0s - loss: 0.6895 - accuracy: 0.8847\n",
            "Epoch 75/100\n",
            "13/13 - 0s - loss: 0.6623 - accuracy: 0.8872\n",
            "Epoch 76/100\n",
            "13/13 - 0s - loss: 0.6282 - accuracy: 0.9048\n",
            "Epoch 77/100\n",
            "13/13 - 0s - loss: 0.6078 - accuracy: 0.8972\n",
            "Epoch 78/100\n",
            "13/13 - 0s - loss: 0.5863 - accuracy: 0.9148\n",
            "Epoch 79/100\n",
            "13/13 - 0s - loss: 0.5759 - accuracy: 0.9173\n",
            "Epoch 80/100\n",
            "13/13 - 0s - loss: 0.5471 - accuracy: 0.9298\n",
            "Epoch 81/100\n",
            "13/13 - 0s - loss: 0.5342 - accuracy: 0.9323\n",
            "Epoch 82/100\n",
            "13/13 - 0s - loss: 0.5215 - accuracy: 0.9373\n",
            "Epoch 83/100\n",
            "13/13 - 0s - loss: 0.4937 - accuracy: 0.9398\n",
            "Epoch 84/100\n",
            "13/13 - 0s - loss: 0.4768 - accuracy: 0.9524\n",
            "Epoch 85/100\n",
            "13/13 - 0s - loss: 0.4537 - accuracy: 0.9674\n",
            "Epoch 86/100\n",
            "13/13 - 0s - loss: 0.4328 - accuracy: 0.9549\n",
            "Epoch 87/100\n",
            "13/13 - 0s - loss: 0.4224 - accuracy: 0.9599\n",
            "Epoch 88/100\n",
            "13/13 - 0s - loss: 0.3992 - accuracy: 0.9699\n",
            "Epoch 89/100\n",
            "13/13 - 0s - loss: 0.3887 - accuracy: 0.9774\n",
            "Epoch 90/100\n",
            "13/13 - 0s - loss: 0.3723 - accuracy: 0.9774\n",
            "Epoch 91/100\n",
            "13/13 - 0s - loss: 0.3599 - accuracy: 0.9799\n",
            "Epoch 92/100\n",
            "13/13 - 0s - loss: 0.3516 - accuracy: 0.9850\n",
            "Epoch 93/100\n",
            "13/13 - 0s - loss: 0.3318 - accuracy: 0.9825\n",
            "Epoch 94/100\n",
            "13/13 - 0s - loss: 0.3258 - accuracy: 0.9825\n",
            "Epoch 95/100\n",
            "13/13 - 0s - loss: 0.3068 - accuracy: 0.9825\n",
            "Epoch 96/100\n",
            "13/13 - 0s - loss: 0.2972 - accuracy: 0.9799\n",
            "Epoch 97/100\n",
            "13/13 - 0s - loss: 0.2892 - accuracy: 0.9825\n",
            "Epoch 98/100\n",
            "13/13 - 0s - loss: 0.2805 - accuracy: 0.9875\n",
            "Epoch 99/100\n",
            "13/13 - 0s - loss: 0.2709 - accuracy: 0.9875\n",
            "Epoch 100/100\n",
            "13/13 - 0s - loss: 0.2667 - accuracy: 0.9799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f922b8d2850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucFzGFIKj8lG"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3RbT8AgGXzS"
      },
      "source": [
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# one hot encode\n",
        "\t\tencoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itIrKKWcGpke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59e2b8a-2a98-4468-a9ea-89f8195e52ea"
      },
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "# test not in original\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))\n",
        "# Test\n",
        "print(generate_seq(model, mapping, 10, 'was in his', 20))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence, A poc\n",
            "king was in his poun d Whee th\n",
            "hello worl,,  hakiid  unnpyy f\n",
            "was in his poun d Whee th eng \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77h1OJ9bsd7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e29ebd7-d8ef-4a33-a3e4-823c0fb15081"
      },
      "source": [
        "y\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t33H1q3J80jv"
      },
      "source": [
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}